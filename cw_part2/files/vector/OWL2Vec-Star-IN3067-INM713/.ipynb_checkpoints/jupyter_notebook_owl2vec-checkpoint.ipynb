{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a6c5ab",
   "metadata": {},
   "source": [
    "## Running OWL2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2658935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ernesto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO: Access the ontology ...\n",
      "INFO: There are 1945 triples in the ontology\n",
      "INFO: Calculate the ontology projection ...\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 1.6564459800720215 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.03980684280395508 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 0.586498498916626 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.022230863571166992 seconds \n",
      "INFO: \tExtracting triples associated to hasBase\n",
      "INFO: \t\tTime extracting triples for property: 0.21331787109375 seconds \n",
      "INFO: \tExtracting triples associated to hasIngredient\n",
      "INFO: \t\tTime extracting triples for property: 0.17404770851135254 seconds \n",
      "INFO: \tExtracting triples associated to isBaseOf\n",
      "INFO: \t\tTime extracting triples for property: 0.17123842239379883 seconds \n",
      "INFO: \tExtracting triples associated to hasCountryOfOrigin\n",
      "INFO: \t\tTime extracting triples for property: 0.1666266918182373 seconds \n",
      "INFO: \tExtracting triples associated to isIngredientOf\n",
      "INFO: \t\tTime extracting triples for property: 0.18834519386291504 seconds \n",
      "INFO: \tExtracting triples associated to hasSpiciness\n",
      "INFO: \t\tTime extracting triples for property: 0.188582181930542 seconds \n",
      "INFO: \tExtracting triples associated to hasTopping\n",
      "INFO: \t\tTime extracting triples for property: 0.244340181350708 seconds \n",
      "INFO: \tExtracting triples associated to isToppingOf\n",
      "INFO: \t\tTime extracting triples for property: 0.16484642028808594 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.0007565021514892578 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 5.318642616271973 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 0.8446321487426758 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: Projection saved into turtle file: ./cache/projection.ttl\n",
      "INFO: Extract classes and individuals ...\n",
      "INFO: Extract axioms ...\n",
      "INFO: Extract annotations ...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ernesto/nltk_data'\n    - '/home/ernesto/Documents/City-INM713-IN3067/py-owl2vec-env/nltk_data'\n    - '/home/ernesto/Documents/City-INM713-IN3067/py-owl2vec-env/share/nltk_data'\n    - '/home/ernesto/Documents/City-INM713-IN3067/py-owl2vec-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mowl2vec_star\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m owl2vec_star\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Parameters:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ontology_file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# config_file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# uri_doc\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# lit_doc\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# mix_doc\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m gensim_model \u001b[38;5;241m=\u001b[39m \u001b[43mowl2vec_star\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_owl2vec_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./case_studies/pizza/pizza.owl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./default.cfg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m output_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./cache/output/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Gensim format\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/OWL2Vec-Star-IN3067-INM713/owl2vec_star/owl2vec_star.py:45\u001b[0m, in \u001b[0;36mextract_owl2vec_model\u001b[0;34m(ontology_file, config_file, uri_doc, lit_doc, mix_doc)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     42\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 45\u001b[0m model_ \u001b[38;5;241m=\u001b[39m \u001b[43m__perform_ontology_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/OWL2Vec-Star-IN3067-INM713/owl2vec_star/owl2vec_star.py:131\u001b[0m, in \u001b[0;36m__perform_ontology_embedding\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    129\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(projection\u001b[38;5;241m.\u001b[39mentityToPreferredLabels[e])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#Keeps only one\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m uri_label[e] \u001b[38;5;241m=\u001b[39m \u001b[43mpre_process_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m##Populates dictionary with all labels per entity\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m projection\u001b[38;5;241m.\u001b[39mgetPreferredLabelsForEntity(e):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m#print(\"Preferred: \" + label)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/OWL2Vec-Star-IN3067-INM713/owl2vec_star/lib/Label.py:28\u001b[0m, in \u001b[0;36mpre_process_words\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpre_process_words\u001b[39m(words):\n\u001b[1;32m     27\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/.*[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn]*\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, word, flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mMULTILINE) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words])\n\u001b[0;32m---> 28\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# processed_tokens = [token.lower() for token in tokens if token.isalpha()]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     processed_tokens \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/py-owl2vec-env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/py-owl2vec-env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/py-owl2vec-env/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/py-owl2vec-env/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/py-owl2vec-env/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/Documents/City-INM713-IN3067/py-owl2vec-env/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ernesto/nltk_data'\n    - '/home/ernesto/Documents/City-INM713-IN3067/py-owl2vec-env/nltk_data'\n    - '/home/ernesto/Documents/City-INM713-IN3067/py-owl2vec-env/share/nltk_data'\n    - '/home/ernesto/Documents/City-INM713-IN3067/py-owl2vec-env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from owl2vec_star import owl2vec_star\n",
    "\n",
    "\n",
    "#Parameters:\n",
    "# ontology_file\n",
    "# config_file\n",
    "# uri_doc\n",
    "# lit_doc\n",
    "# mix_doc\n",
    "gensim_model = owl2vec_star.extract_owl2vec_model(\"./case_studies/pizza/pizza.owl\", \"./default.cfg\", True, True, True)\n",
    "\n",
    "output_folder=\"./cache/output/\"\n",
    "\n",
    "#Gensim format\n",
    "gensim_model.save(output_folder+\"ontology.embeddings\")\n",
    "    #Txt format\n",
    "gensim_model.wv.save_word2vec_format(output_folder+\"ontology.embeddings.txt\", binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a78df2",
   "metadata": {},
   "source": [
    "## Loading embeddings and getting similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdbcf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: loading KeyedVectors object from ./cache/output/ontology.embeddings\n",
      "INFO: loading wv recursively from ./cache/output/ontology.embeddings.wv.* with mmap=r\n",
      "INFO: setting ignored attribute cum_table to None\n",
      "INFO: Word2Vec lifecycle event {'fname': './cache/output/ontology.embeddings', 'datetime': '2025-03-27T17:50:59.373945', 'gensim': '4.3.1', 'python': '3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]', 'platform': 'Linux-6.8.0-52-generic-x86_64-with-glibc2.35', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'pizza'\n",
      "[-0.1945018   0.08851976 -0.15513498  0.3381694  -0.2129597   0.12973897\n",
      " -0.15032986 -0.1092433  -0.3558837  -0.5600534   0.10304057  0.513794\n",
      "  0.17977324 -0.18524191  0.57300913 -0.15506467 -0.11119636  0.05171384\n",
      " -0.2346962   0.12931794  0.1665135  -0.36509317  0.06428502  0.68987495\n",
      "  0.18555167 -0.53496236 -0.00496973 -0.5405587  -0.05750511  0.22604577\n",
      "  0.30401808 -0.32775784 -0.2545755  -0.01071084  0.38638088  0.43401417\n",
      "  0.07623713 -0.15894029 -0.40949675  0.01086321  0.11000457 -0.18090476\n",
      " -0.0802938  -0.12670936  0.09813047 -0.05658809  0.2252289   0.42154148\n",
      "  0.02729056 -0.34647375  0.5734723  -0.24233647 -0.11629723 -0.0957427\n",
      "  0.5619853  -0.02067205 -0.26408824  0.3518525  -0.14176178  0.13746543\n",
      "  0.58949494  0.17152871 -0.4650318  -0.06581306 -0.11346283  0.11281835\n",
      " -0.12107798 -0.37187967 -0.40153283  0.18688595 -0.00416304 -0.5961161\n",
      " -0.3100173  -0.66859454  0.08155271 -0.1169589   0.10241111 -0.17001437\n",
      " -0.05869089 -0.33182544 -0.22666226  0.50473726  0.30803382  0.28058589\n",
      " -0.31465545 -0.08549416 -0.62776166 -0.07590489  0.3443104  -0.11723706\n",
      "  0.96925163 -0.20252012 -0.36418965  0.50043833  0.06101945  0.65459317\n",
      "  0.01740384 -0.25508642 -0.04075726 -0.3381781 ]\n",
      "0.4155082\n",
      "0.788168\n",
      "[('unclosedpizza', 0.8107369542121887), ('margherita pizza', 0.8077318072319031), ('parmese pizza', 0.7995721697807312), ('rosa pizza', 0.7844170928001404), ('caprina pizza', 0.7776142358779907), ('la reine pizza', 0.7715996503829956), ('sundried tomato', 0.7654079794883728), ('american pizza', 0.7606872916221619), ('soho pizza', 0.7562614679336548), ('napoletana pizza', 0.7549246549606323)]\n",
      "[('unclosedpizza', 0.9166375398635864), ('unclosed', 0.9089506268501282), ('margherita pizza', 0.8959925174713135), ('http://www.co-ode.org/ontologies/pizza/pizza.owl#Margherita', 0.8940831422805786), ('caprina pizza', 0.8751577138900757), ('parmese pizza', 0.8743725419044495), ('rosa pizza', 0.8721114993095398), ('Margherita Pizza', 0.8707189559936523), ('american pizza', 0.8706991672515869), ('rosa', 0.8701453804969788)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Embedding vectors generated above\n",
    "model = KeyedVectors.load(\"./cache/output/ontology.embeddings\", mmap='r')\n",
    "wv = model.wv\n",
    "\n",
    "vector = wv['pizza']  # Get numpy vector of a word\n",
    "print(\"Vector for 'pizza'\")\n",
    "print(vector)\n",
    "\n",
    "#cosine similarity\n",
    "similarity = wv.similarity('pizza', 'http://www.co-ode.org/ontologies/pizza/pizza.owl#Pizza')\n",
    "print(similarity)\n",
    "\n",
    "similarity = wv.similarity('http://www.co-ode.org/ontologies/pizza/pizza.owl#Margherita', 'margherita')\n",
    "print(similarity)\n",
    "\n",
    "\n",
    "#Most similar cosine similarity\n",
    "result = wv.most_similar(positive=['margherita', 'pizza'])\n",
    "print(result)\n",
    "\n",
    "#Most similar entities: cosmul\n",
    "result = wv.most_similar_cosmul(positive=['margherita'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdbe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
